# -*- coding: utf-8 -*-
"""train_my_yolo_model .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-YqY59WQrl0hp-TkQb-tXcRA6l4Si2u
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/gdrive')

!cp /content/custom_data.zip /content

# Unzip images to a custom data folder
!unzip -q /content/custom_data.zip -d /content/custom_data

# Python function to automatically create data.yaml config file
# 1. Reads "classes.txt" file to get list of class names
# 2. Creates data dictionary with correct paths to folders, number of classes, and names of classes
# 3. Writes data in YAML format to data.yaml

import yaml
import os

def create_data_yaml(path_to_classes_txt, path_to_data_yaml):

  # Read class.txt to get class names
  if not os.path.exists(path_to_classes_txt):
    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')
    return
  with open(path_to_classes_txt, 'r') as f:
    classes = []
    for line in f.readlines():
      if len(line.strip()) == 0: continue
      classes.append(line.strip())
  number_of_classes = len(classes)

  # Create data dictionary
  data = {
      'path': '/content/data',
      'train': 'train/images',
      'val': 'validation/images',
      'nc': number_of_classes,
      'names': classes
  }

  # Write data to YAML file
  with open(path_to_data_yaml, 'w') as f:
    yaml.dump(data, f, sort_keys=False)
  print(f'Created config file at {path_to_data_yaml}')

  return

# Define path to classes.txt and run function
path_to_classes_txt = '/content/custom_data/custom_data/classes.txt'
path_to_data_yaml = '/content/data.yaml'

create_data_yaml(path_to_classes_txt, path_to_data_yaml)

print('\nFile contents:\n')
!cat /content/data.yaml

"""## 4. Install Requirements (Ultralytics)

Next, we'll install the Ultralytics library in this Google Colab instance. This Python library will be used to train the YOLO model.
"""

!pip install ultralytics

"""## 5. Train Model

5.1 Training Parameters
Now that the data is organized and the config file is created, we're ready to start training! First, there are a few important parameters to decide on. Visit my article on Training YOLO Models Locally to learn more about these parameters and how to choose them.

Model architecture & size (model):

There are several YOLOv12 models sizes available to train, including yolov12n.pt, yolov12s.pt, yolo11m.pt. Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. yolov12n.pt is a good starting point.


Number of epochs (epochs)

In machine learning, one “epoch” is one single pass through the full training dataset. Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset and the model architecture. If your dataset has less than 200 images, a good starting point is 60 epochs. If your dataset has more than 200 images, a good starting point is 40 epochs.

Resolution (imgsz)

Resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. YOLO models are typically trained and inferenced at a 640x640 resolution. However, if you want your model to run faster or know you will be working with low-resolution images, try using a lower resolution like 480x480.

# 5.2 Run Training!
"""

import os

paths = [
    "/content/data/train/images",
    "/content/data/train/labels",
    "/content/data/validation/images",
    "/content/data/validation/labels"
]

for p in paths:
    print(f"{p}: {'✅ Found' if os.path.exists(p) else '❌ MISSING'}")

"""Run the following code block to begin training. If you want to use a different model, number of epochs, or resolution, change model, epochs, or imgsz."""

!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=60 imgsz=640

"""## 6. Test Model"""

!yolo detect predict model=runs/detect/train2/weights/best.pt source=/content/custom_data/custom_data/validation/images save=True

import glob
from IPython.display import Image, display
for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:30]:
  display(Image(filename=image_path, height=400))
  print('\n')

"""Below code loads your custom trained model, runs object detection on a video file uploaded to Colab, and saves the output video with detections."""

from ultralytics import YOLO

# Replace with the path to your custom trained model file (e.g., custom_model.pt)
model_path = "/content/runs/detect/train/weights/best.pt"

# Replace with the path to your video file in Colab
video_path = "/content/video.mp4"

# Load the custom model
model = YOLO(model_path)

# Run detection on the video
# 'save=True' saves the annotated video in the default output directory (typically 'runs/detect/predict')
results = model.predict(source=video_path, save=True, conf=0.25, imgsz=640)

print("Detection complete! Check the 'runs/detect/predict' folder for the annotated video.")

"""convert .avi to .mp4"""

!ffmpeg -i /content/runs/detect/predict2/video.avi -c:v libx264 -crf 23 -preset veryfast -c:a aac -b:a 192k /content/runs/detect/predict2/video.mp4

from IPython.display import HTML
import base64

def display_video(file_path):
    with open(file_path, "rb") as f:
        video_data = f.read()
    video_encoded = base64.b64encode(video_data).decode("utf-8")
    video_html = f"""
    <video width="640" height="480" controls>
      <source src="data:video/mp4;base64,{video_encoded}" type="video/mp4">
    </video>
    """
    return HTML(video_html)

# Replace with the path to your converted MP4 file
display_video("/content/runs/detect/predict2/video.mp4")

"""not used block"""

import os
from IPython.display import HTML
from base64 import b64encode

# Run detection on the video
# 'save=True' will output the annotated video into the default runs folder.
#results = model.predict(source=video_path, save=True, conf=0.25, imgsz=640)

# The annotated video is usually saved in 'runs/detect/predict'
annotated_video_dir = "/content/runs/detect/predict2"
mp4_file = None
for f in os.listdir(annotated_video_dir):
    if f.endswith(".avi"):
        mp4_file = os.path.join(annotated_video_dir, f)
        break

if mp4_file is None:
    print("No annotated MP4 video file was found.")
else:
    print(f"Annotated video saved at: {mp4_file}")

    # Function to display video inline using HTML5
    def display_video(file_path):
        video_file = open(file_path, "rb").read()
        video_encoded = b64encode(video_file).decode("utf-8")
        html = f"""
        <video width="640" height="480" controls>
          <source src="data:video/mp4;base64,{video_encoded}" type="video/mp4">
        </video>
        """
        return HTML(html)

    # Display the video in the Colab notebook
    display_video(mp4_file)